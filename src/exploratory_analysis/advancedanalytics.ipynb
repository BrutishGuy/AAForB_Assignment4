{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing the size of the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Separation of queries: \n",
    "We should find a way to return graphs of posts, users and subreddits, one subreddit at a time. This is computationally easier to handle for our pc's. The queries will be simpler, and it will be more manual work. However once we have all these separate files we can merge them together into a big connected graph. It is not clear to me yet which approach is easiest.\n",
    "1.  Exporting all the data in a csv and merging the csv's together and importing the data into gephi OR\n",
    "2.  Exporting the graphml files and merging them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Reducing Complexity of the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 User short list\n",
    "We generated two approaches to arrive to a shortlist of users to analyse\n",
    "\n",
    "#### 2.1.1 Subreddit driven approach: \n",
    "For each of the three main corona subreddits we need to return the top 100 posts with the highest karma. For these 300 posts we extract the users that made them. These are intensive reddit users that have a high Impact within these three subreddits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**NEO4J QUERY**\n",
    "\n",
    "**coronavirus**\n",
    "```\n",
    "MATCH (u:User)--(p:Post)--(s:Subreddit {display_name:'coronavirus'})\n",
    "RETURN p.title, p.score,  s.display_name,u.username\n",
    "ORDER BY p.score DESC\n",
    "LIMIT 100\n",
    "```\n",
    "\n",
    "**covid19**\n",
    "```\n",
    "MATCH (u:User)--(p:Post)--(s:Subreddit {display_name:'coronavirus'})\n",
    "RETURN p.title, p.score,  s.display_name,u.username\n",
    "ORDER BY p.score DESC\n",
    "LIMIT 100\n",
    "```\n",
    "\n",
    "**china_flu**\n",
    "```\n",
    "MATCH (u:User)--(p:Post)--(s:Subreddit {display_name:'coronavirus'})\n",
    "RETURN p.title, p.score,  s.display_name,u.username\n",
    "ORDER BY p.score DESC\n",
    "LIMIT 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function defintions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates(df):\n",
    "    \"Takes UTC dates and returns columns for the months, weeks \"\n",
    "    df['month'] = pd.to_datetime(arg=df.loc[:, 'date']).dt.month_name()\n",
    "    df['month_n'] = pd.to_datetime(arg=df.loc[:, 'date']).dt.month\n",
    "    df['day'] = pd.to_datetime(arg=df.loc[:, 'date']).dt.day_name()\n",
    "    df['day_y'] = pd.to_datetime(arg=df.loc[:, 'date']).dt.dayofyear\n",
    "    df['day_w'] = pd.to_datetime(arg=df.loc[:, 'date']).dt.dayofweek\n",
    "    df['week_y'] = pd.to_datetime(arg=df.loc[:, 'date']).dt.weekofyear\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract list of users from the three main corona subreddits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users shape: (300, 4)\n",
      "               score\n",
      "count     300.000000\n",
      "mean    22307.620000\n",
      "std     29433.558186\n",
      "min       664.000000\n",
      "25%      1690.750000\n",
      "50%      3105.000000\n",
      "75%     49307.500000\n",
      "max    110424.000000 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 300 entries, 0 to 99\n",
      "Data columns (total 4 columns):\n",
      "title        300 non-null object\n",
      "score        300 non-null int64\n",
      "subreddit    300 non-null object\n",
      "username     300 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 11.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'C:\\\\Users\\\\delah\\\\Documents\\\\Programming\\\\workspace\\\\AAForB_Assignment4\\\\data'\n",
    "ANALYSIS_DIR = '\\\\users'\n",
    "INPUT_DIR = '\\\\input'\n",
    "OUTPUT_DIR = '\\\\output'\n",
    "VIS_DIR = 'C:\\\\Users\\\\delah\\\\Documents\\\\Programming\\\\workspace\\\\AAForB_Assignment4\\\\outputs\\\\exploratory_analysis\\\\users\\\\'\n",
    "\n",
    "os.chdir(DATA_DIR + ANALYSIS_DIR + INPUT_DIR)\n",
    "\n",
    "files = ['china_flu','coronavirus','covid19']\n",
    "\n",
    "users = pd.DataFrame()\n",
    "for file in files:\n",
    "    temp = pd.read_csv(file)\n",
    "    users = users.append(temp)\n",
    "    \n",
    "users = users.rename(columns={'p.title':'title','s.display_name':'subreddit','p.score':'score','u.username':'username'})\n",
    "\n",
    "\n",
    "usernames = users.username.values\n",
    "#print(usernames)  # print this to get the full list in Neo4J compatible format\n",
    "#users = users.iloc[0:50,:] # We reduce the list of users to limit the size of the final graph down the line\n",
    "subreddit_driven = users\n",
    "\n",
    "\n",
    "print(\"Users list shape:\", users.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save data to directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(DATA_DIR + ANALYSIS_DIR + OUTPUT_DIR)\n",
    "users.to_csv('subreddit_driven.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 user driven approach: \n",
    "Return the list of users that have posted in all three subreddits, Of this list, take those with the highest karma.\n",
    "There will be some overlap in users between the subreddit driven approach and the user driven approach which is good. The former emphasize highly contributing members within each community, while the latter emphasizes on members that connect communities (with or without reposts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEO4J QUERY**\n",
    "```\n",
    "match (sr:Subreddit)--(p:Post)--(u:User)\n",
    "where sr.display_name in ['coronavirus', 'covid19', 'china_flu']\n",
    "and u.username <> 'AutoModerator'\n",
    "with distinct u as myUsers \n",
    "\n",
    "match (myUsers:User)--(pAny:Post)--(srAny:Subreddit)\n",
    "where srAny.display_name in ['europe', 'lifeprotips', 'science', 'videos', 'technology', 'iama', 'todayilearned', 'coronavirus', 'askreddit', 'explainlikeimfive', 'news', 'china_flu', 'covid19', 'nottheonion', 'politics', 'upliftingnews', 'askscience', 'worldnews', 'dataisbeautiful']\n",
    "with myUsers as my_N_Users, count(distinct pAny) as cntDiffPosts\n",
    "order by cntDiffPosts desc, my_N_Users.username limit 300\n",
    "\n",
    "return my_N_Users.username\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users shape: (300, 2)\n",
      "       post_count\n",
      "count   300.00000\n",
      "mean    300.98000\n",
      "std     354.25997\n",
      "min     138.00000\n",
      "25%     171.75000\n",
      "50%     215.00000\n",
      "75%     299.50000\n",
      "max    4900.00000 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300 entries, 0 to 299\n",
      "Data columns (total 2 columns):\n",
      "username      300 non-null object\n",
      "post_count    300 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 4.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "os.chdir(DATA_DIR + ANALYSIS_DIR + OUTPUT_DIR)\n",
    "\n",
    "file = 'user_driven.csv'\n",
    "\n",
    "users = pd.read_csv(file)\n",
    "    \n",
    "users = users.rename(columns={'my_N_Users':'username','cntDiffPosts':'post_count'})\n",
    "\n",
    "\n",
    "\n",
    "usernames = users.username.values\n",
    "#print(usernames)  # print this to get the full list in Neo4J compatible format\n",
    "\n",
    "#users = users.iloc[0:60,:]\n",
    "user_driven = users\n",
    "\n",
    "print(\"Users shape:\", users.shape)\n",
    "print(users.describe(), '\\n')\n",
    "print(users.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the user driven approach list:300\n",
      "Length of the subreddit driven approach list300\n",
      "Length of both lists:600\n",
      "Length of both lists when removing duplicates512\n"
     ]
    }
   ],
   "source": [
    "print('Length of the user driven approach list:' + str(len(user_driven.loc[:,['username']])))\n",
    "print('Length of the subreddit driven approach list' + str(len(subreddit_driven.loc[:,['username']])))\n",
    "users_master_list = user_driven.loc[:,['username']].append(subreddit_driven.loc[:,['username']])\n",
    "print('Length of both lists:' + str(len(users_master_list)))\n",
    "print('Length of both lists when removing duplicates' + str(len(set(users_master_list.values.flatten()))))\n",
    "os.chdir(DATA_DIR + ANALYSIS_DIR + OUTPUT_DIR)\n",
    "users_master_list.to_csv('users_master_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "usernames = list(set(users_master_list.values.flatten())) # print this to get the full list in Neo4J compatible format\n",
    "print(len(usernames))\n",
    "#usernames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 subreddits: \n",
    "There are too many subreddits to query. Even with our user's shorts, if we query all the posts they make and their respective subreddits we will have a data explosion. Only limiting ourselves to the three main subreddits seems too\n",
    "limiting too. We limit our queries to the subreddits from a new shortlist I made based on the \"Biggest subreddits\" list I posted above. The new list is shorted and contains the 7 subreddits that have the most posts related to corona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subreddits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-54b976d32c47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msubreddits\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'subreddits' is not defined"
     ]
    }
   ],
   "source": [
    "subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't forget to add datascience and machinelearning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 posts:\n",
    "While many subreddits like /r/news have fewer than 5000 posts made for the entire period of analysis, others like /r/coronavirus have 5000 posts just for the april period. Again the goal is here to filter on posts that cover the coronacrisis.\n",
    "However we want to limit that pool even further to reduce the total complexity of the file.\n",
    "       @François Chandelle Let me know what you think of this.\n",
    "#### 2.3.1 for the three mainsubreddits:\n",
    "we export all the posts made by our shortlist of users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEO4J QUERIES:**\n",
    "\n",
    "**coronvirus**\n",
    "```\n",
    "MATCH (u:User)-[e2]-(p:Post)-[e1]-(s:Subreddit{display_name:'coronavirus'})\n",
    "WHERE u.username in //Add master list of users here\n",
    "RETURN u.id, u.username, u.link_karma, u.comment_karma , s.id, s.display_name,p.id, p.created_utc_str, p.score, p.upvote_ratio ,p.title\n",
    "ORDER BY p.score DESC\n",
    "LIMIT 10000\n",
    "```\n",
    "\n",
    "\n",
    "**covid19**\n",
    "```\n",
    "MATCH (u:User)-[e2]-(p:Post)-[e1]-(s:Subreddit{display_name:'covid19'})\n",
    "WHERE u.username in //Add master list of users here\n",
    "RETURN u.id, u.username, u.link_karma, u.comment_karma , s.id, s.display_name,p.id, p.created_utc_str, p.score, p.upvote_ratio ,p.title\n",
    "ORDER BY p.score DESC\n",
    "LIMIT 10000\n",
    "```\n",
    "\n",
    "**china_flu**\n",
    "```\n",
    "MATCH (u:User)-[e2]-(p:Post)-[e1]-(s:Subreddit{display_name:'china_flu'})\n",
    "WHERE u.username in //Add master list of users here\n",
    "RETURN u.id, u.username, u.link_karma, u.comment_karma , s.id, s.display_name,p.id, p.created_utc_str, p.score, p.upvote_ratio ,p.title\n",
    "ORDER BY p.score DESC\n",
    "LIMIT 10000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subreddits extracted: 19\n",
      "['iama', 'explainlikeimfive', 'technology', 'covid19', 'askscience', 'askreddit', 'china_flu', 'politics', 'dataisbeautiful', 'europe', 'videos', 'worldnews', 'coronavirus', 'upliftingnews', 'nottheonion', 'todayilearned', 'news', 'science', 'lifeprotips']\n",
      "(32303, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>month</th>\n",
       "      <th>month_n</th>\n",
       "      <th>day</th>\n",
       "      <th>day_y</th>\n",
       "      <th>day_w</th>\n",
       "      <th>week_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Redditors bored because coronavirus cancelled ...</td>\n",
       "      <td>askreddit</td>\n",
       "      <td>2020-03-13 15:17:31</td>\n",
       "      <td>3</td>\n",
       "      <td>March</td>\n",
       "      <td>3</td>\n",
       "      <td>Friday</td>\n",
       "      <td>73</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why would a second coronavirus wave breakout i...</td>\n",
       "      <td>askreddit</td>\n",
       "      <td>2020-04-13 05:58:50</td>\n",
       "      <td>1</td>\n",
       "      <td>April</td>\n",
       "      <td>4</td>\n",
       "      <td>Monday</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What good things are happening right now but i...</td>\n",
       "      <td>askreddit</td>\n",
       "      <td>2020-04-15 00:10:04</td>\n",
       "      <td>1</td>\n",
       "      <td>April</td>\n",
       "      <td>4</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>106</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So now, what is the most memorable fake news y...</td>\n",
       "      <td>askreddit</td>\n",
       "      <td>2020-03-28 21:16:05</td>\n",
       "      <td>1</td>\n",
       "      <td>March</td>\n",
       "      <td>3</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>people who had to use the health system during...</td>\n",
       "      <td>askreddit</td>\n",
       "      <td>2020-03-23 23:45:51</td>\n",
       "      <td>1</td>\n",
       "      <td>March</td>\n",
       "      <td>3</td>\n",
       "      <td>Monday</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  subreddit  \\\n",
       "0  Redditors bored because coronavirus cancelled ...  askreddit   \n",
       "1  Why would a second coronavirus wave breakout i...  askreddit   \n",
       "2  What good things are happening right now but i...  askreddit   \n",
       "3  So now, what is the most memorable fake news y...  askreddit   \n",
       "4  people who had to use the health system during...  askreddit   \n",
       "\n",
       "                  date  score  month  month_n        day  day_y  day_w  week_y  \n",
       "0  2020-03-13 15:17:31      3  March        3     Friday     73      4      11  \n",
       "1  2020-04-13 05:58:50      1  April        4     Monday    104      0      16  \n",
       "2  2020-04-15 00:10:04      1  April        4  Wednesday    106      2      16  \n",
       "3  2020-03-28 21:16:05      1  March        3   Saturday     88      5      13  \n",
       "4  2020-03-23 23:45:51      1  March        3     Monday     83      0      13  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Point to directory containing data\n",
    "DATA_DIR = 'C:\\\\Users\\\\delah\\\\Documents\\\\Programming\\\\workspace\\\\AAForB_Assignment4\\\\data'\n",
    "ANALYSIS_DIR = '\\\\coronavirustrends'\n",
    "INPUT_DIR = '\\\\input'\n",
    "OUTPUT_DIR = '\\\\output'\n",
    "VIS_DIR = 'C:\\\\Users\\\\delah\\\\Documents\\\\Programming\\\\workspace\\\\AAForB_Assignment4\\\\outputs\\\\exploratory_analysis\\\\coronatrends\\\\'\n",
    "\n",
    "\n",
    "os.chdir(DATA_DIR + ANALYSIS_DIR + INPUT_DIR)\n",
    "\n",
    "# Importing data and renaming columns\n",
    "df = pd.DataFrame()\n",
    "for i in range(1,20):\n",
    "    temp = pd.read_csv(str(i))\n",
    "    df = df.append(temp)\n",
    "    \n",
    "df = df.rename(columns={'p.title':'title','s.display_name':'subreddit','p.created_utc_str':'date','p.score':'score'})\n",
    "\n",
    "# Extract subreddits\n",
    "subreddits = list(set(df.loc[:,['subreddit']].values.flatten()))\n",
    "print(\"Number of subreddits extracted:\", len(list(subreddits)))\n",
    "print(subreddits)\n",
    "\n",
    "# Get months, weeks and days\n",
    "df = dates(df)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37386, 23)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['u.id', 'username', 'link_karma', 'comment_karma', 's.id', 'subreddit',\n",
       "       'p.id', 'date', 'score', 'upvote_ratio', 'title', 'month', 'month_n',\n",
       "       'day', 'day_y', 'day_w', 'week_y', 'total_posts_per_user',\n",
       "       'total_score_per_user', 'total_posts_per_subreddit',\n",
       "       'total_scores_per_subreddit', 'average_karma_per_post',\n",
       "       'Time Interval'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Setting the directories\n",
    "DATA_DIR = 'C:\\\\Users\\\\delah\\\\Documents\\\\Programming\\\\workspace\\\\AAForB_Assignment4\\\\data'\n",
    "ANALYSIS_DIR = '\\\\posts'\n",
    "INPUT_DIR = '\\\\input'\n",
    "OUTPUT_DIR = '\\\\output'\n",
    "VIS_DIR = 'C:\\\\Users\\\\delah\\\\Documents\\\\Programming\\\\workspace\\\\AAForB_Assignment4\\\\outputs\\\\exploratory_analysis\\\\posts\\\\'\n",
    "os.chdir(DATA_DIR + ANALYSIS_DIR + INPUT_DIR)\n",
    "\n",
    "# Loading the data\n",
    "\n",
    "files = ['china_flu_complete', 'coronavirus_complete', 'covid19_complete','technology',  'nottheonion',  'videos',  'politics',  'dataisbeautiful',  'askscience',  'news',  'askreddit',  'worldnews',  'todayilearned',  'upliftingnews',  'science',  'explainlikeimfive',  'europe']\n",
    "#files = ['china_flu', 'coronavirus', 'covid19','technology',  'nottheonion',  'videos',  'politics',  'dataisbeautiful',  'askscience',  'news',  'askreddit',  'worldnews',  'todayilearned',  'upliftingnews',  'science',  'explainlikeimfive',  'europe'] # francois version with all posts from the main three\n",
    "posts = pd.DataFrame()\n",
    "for file in files:\n",
    "    temp = pd.read_csv(file)\n",
    "    posts = posts.append(temp)\n",
    "\n",
    "#Cleaning posts dataframe\n",
    "posts = posts.rename(columns={'p.title':'title','s.display_name':'subreddit','p.score':'score','u.username':'username','u.link_karma':'link_karma','u.comment_karma':'comment_karma', 'p.created_utc_str':'date','p.upvote_ratio':'upvote_ratio'})\n",
    "posts = dates(posts)\n",
    "\n",
    "#Groupby's \n",
    "# Users\n",
    "total_posts_per_user = posts.loc[:,['username','title']].groupby('username').count().sort_values(by='title',  ascending=False)\n",
    "total_posts_per_user = total_posts_per_user.rename(columns={'title':'total_posts_per_user'}).reset_index() \n",
    "total_scores_per_user = posts.loc[:,['username','score']].groupby('username').sum().sort_values(by='score', ascending=False).reset_index() \n",
    "total_scores_per_user = total_scores_per_user.rename(columns={'score':'total_score_per_user'})\n",
    "\n",
    "#Subreddit \n",
    "total_posts_per_subreddit = posts.loc[:,['subreddit','title']].groupby('subreddit').count().sort_values(by='title',  ascending=False)\n",
    "total_posts_per_subreddit = total_posts_per_subreddit.rename(columns={'title':'total_posts_per_subreddit'}).reset_index() \n",
    "total_scores_per_subreddit = posts.loc[:,['subreddit','score']].groupby('subreddit').sum().sort_values(by='score', ascending=False).reset_index() \n",
    "total_scores_per_subreddit = total_scores_per_subreddit.rename(columns={'score':'total_scores_per_subreddit'})\n",
    "\n",
    "posts = pd.merge(left=posts,right=total_posts_per_user,how='left',left_on='username',right_on='username')\n",
    "posts = pd.merge(left=posts,right=total_scores_per_user,how='left',left_on='username',right_on='username')\n",
    "posts = pd.merge(left=posts,right=total_posts_per_subreddit,how='left',left_on='subreddit',right_on='subreddit')\n",
    "posts = pd.merge(left=posts,right=total_scores_per_subreddit,how='left',left_on='subreddit',right_on='subreddit')\n",
    "posts['average_karma_per_post'] = posts.loc[:,'total_score_per_user'] / posts.loc[:,'total_posts_per_user']\n",
    "posts.columns\n",
    "\n",
    "\n",
    "#Saving Dataframe\n",
    "os.chdir(DATA_DIR + ANALYSIS_DIR + OUTPUT_DIR)\n",
    "#posts = posts.loc[posts.username.isin(values=usernames)] # Special filter on shorter user list\n",
    "posts['Time Interval'] = posts.loc[:,'day_y']\n",
    "posts.to_csv('complete_graph.csv')\n",
    "print(posts.shape)\n",
    "posts.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final DataFrame containing topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u.id</th>\n",
       "      <th>username</th>\n",
       "      <th>link_karma</th>\n",
       "      <th>comment_karma</th>\n",
       "      <th>s.id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>p.id</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>day_w</th>\n",
       "      <th>week_y</th>\n",
       "      <th>total_posts_per_user</th>\n",
       "      <th>total_score_per_user</th>\n",
       "      <th>total_posts_per_subreddit</th>\n",
       "      <th>total_scores_per_subreddit</th>\n",
       "      <th>average_karma_per_post</th>\n",
       "      <th>avg_sentiment</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2o0ngbg8</td>\n",
       "      <td>goddessofthebitches</td>\n",
       "      <td>8376</td>\n",
       "      <td>1346</td>\n",
       "      <td>2dar36</td>\n",
       "      <td>china_flu</td>\n",
       "      <td>fhxap2</td>\n",
       "      <td>2020-03-13T10:05:33Z</td>\n",
       "      <td>16818</td>\n",
       "      <td>0.85</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>16818</td>\n",
       "      <td>9515</td>\n",
       "      <td>804324</td>\n",
       "      <td>16818.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>American Politics and News</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tblkklw</td>\n",
       "      <td>DeWallenVanWimKok</td>\n",
       "      <td>21356</td>\n",
       "      <td>6296</td>\n",
       "      <td>2dar36</td>\n",
       "      <td>china_flu</td>\n",
       "      <td>f0p5nc</td>\n",
       "      <td>2020-02-08T08:59:21Z</td>\n",
       "      <td>11927</td>\n",
       "      <td>0.95</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>157</td>\n",
       "      <td>27710</td>\n",
       "      <td>9515</td>\n",
       "      <td>804324</td>\n",
       "      <td>176.496815</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>Medical Research and Vaccine</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14mosq</td>\n",
       "      <td>staplehill</td>\n",
       "      <td>21357</td>\n",
       "      <td>42254</td>\n",
       "      <td>2dar36</td>\n",
       "      <td>china_flu</td>\n",
       "      <td>fbt49e</td>\n",
       "      <td>2020-03-01T12:01:27Z</td>\n",
       "      <td>8922</td>\n",
       "      <td>0.98</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>12187</td>\n",
       "      <td>9515</td>\n",
       "      <td>804324</td>\n",
       "      <td>870.500000</td>\n",
       "      <td>0.103280</td>\n",
       "      <td>Medical Research and Vaccine</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14bafgnj</td>\n",
       "      <td>IcyPresence96</td>\n",
       "      <td>4160</td>\n",
       "      <td>535</td>\n",
       "      <td>2dar36</td>\n",
       "      <td>china_flu</td>\n",
       "      <td>feupgd</td>\n",
       "      <td>2020-03-07T12:49:04Z</td>\n",
       "      <td>7043</td>\n",
       "      <td>0.91</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>7074</td>\n",
       "      <td>9515</td>\n",
       "      <td>804324</td>\n",
       "      <td>1768.500000</td>\n",
       "      <td>0.232119</td>\n",
       "      <td>American Politics and News</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wo4fw</td>\n",
       "      <td>madman320</td>\n",
       "      <td>48415</td>\n",
       "      <td>22054</td>\n",
       "      <td>2dar36</td>\n",
       "      <td>china_flu</td>\n",
       "      <td>f30lyn</td>\n",
       "      <td>2020-02-12T23:49:03Z</td>\n",
       "      <td>6811</td>\n",
       "      <td>0.96</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>168</td>\n",
       "      <td>33525</td>\n",
       "      <td>9515</td>\n",
       "      <td>804324</td>\n",
       "      <td>199.553571</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>Statistics Reporting</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       u.id             username  link_karma  comment_karma    s.id  \\\n",
       "0  2o0ngbg8  goddessofthebitches        8376           1346  2dar36   \n",
       "1   tblkklw    DeWallenVanWimKok       21356           6296  2dar36   \n",
       "2    14mosq           staplehill       21357          42254  2dar36   \n",
       "3  14bafgnj        IcyPresence96        4160            535  2dar36   \n",
       "4     wo4fw            madman320       48415          22054  2dar36   \n",
       "\n",
       "   subreddit    p.id                  date  score  upvote_ratio     ...       \\\n",
       "0  china_flu  fhxap2  2020-03-13T10:05:33Z  16818          0.85     ...        \n",
       "1  china_flu  f0p5nc  2020-02-08T08:59:21Z  11927          0.95     ...        \n",
       "2  china_flu  fbt49e  2020-03-01T12:01:27Z   8922          0.98     ...        \n",
       "3  china_flu  feupgd  2020-03-07T12:49:04Z   7043          0.91     ...        \n",
       "4  china_flu  f30lyn  2020-02-12T23:49:03Z   6811          0.96     ...        \n",
       "\n",
       "  day_w week_y  total_posts_per_user total_score_per_user  \\\n",
       "0     4     11                     1                16818   \n",
       "1     5      6                   157                27710   \n",
       "2     6      9                    14                12187   \n",
       "3     5     10                     4                 7074   \n",
       "4     2      7                   168                33525   \n",
       "\n",
       "   total_posts_per_subreddit  total_scores_per_subreddit  \\\n",
       "0                       9515                      804324   \n",
       "1                       9515                      804324   \n",
       "2                       9515                      804324   \n",
       "3                       9515                      804324   \n",
       "4                       9515                      804324   \n",
       "\n",
       "   average_karma_per_post  avg_sentiment                         topic  \\\n",
       "0            16818.000000       0.000000    American Politics and News   \n",
       "1              176.496815       0.150000  Medical Research and Vaccine   \n",
       "2              870.500000       0.103280  Medical Research and Vaccine   \n",
       "3             1768.500000       0.232119    American Politics and News   \n",
       "4              199.553571       0.374166          Statistics Reporting   \n",
       "\n",
       "   topic_score  \n",
       "0         57.0  \n",
       "1         47.0  \n",
       "2         33.0  \n",
       "3         64.0  \n",
       "4         73.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the directories\n",
    "DATA_DIR = 'C:\\\\Users\\\\delah\\\\Documents\\\\Programming\\\\workspace\\\\AAForB_Assignment4\\\\data'\n",
    "ANALYSIS_DIR = '\\\\posts'\n",
    "INPUT_DIR = '\\\\input\\\\'\n",
    "OUTPUT_DIR = '\\\\output'\n",
    "import os\n",
    "os.chdir(DATA_DIR + ANALYSIS_DIR + INPUT_DIR)\n",
    "final_graph = pd.read_csv('reddit_posts_with_topic_and_sentiment_4.csv',encoding='iso-8859-1')\n",
    "final_graph.drop(labels=['Unnamed: 0'], axis=1, inplace=True)\n",
    "final_graph.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 for the other subredditss: \n",
    "We only export the posts made by our shortlist of users containing topics extracted from the LDA made by victor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gephi converter\n",
    "To create the final graph we need to convert our data to csv. We will need two types of files, csv containing the node infromation and csv's containing the relationship information. Below we create a csv for each\n",
    "\n",
    "\n",
    "## Nodes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delah\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "# Creating the nodes dataframe\n",
    "nodes = final_graph.copy()\n",
    "\n",
    "#Processing nodes\n",
    "# Post nodes\n",
    "nodes_post = nodes.loc[:,['p.id','title','date','score','upvote_ratio','month','month_n','day','Time Interval','day_w','week_y','avg_sentiment', 'topic', 'topic_score']]\n",
    "nodes_post = nodes_post.rename(columns={'p.id':'Id','title':'Label'})\n",
    "nodes_post.loc[:,'Label'] = ' '\n",
    "nodes_post['node_type'] = 'post'\n",
    "nodes_post = nodes_post.set_index(keys='Id')\n",
    "\n",
    "\n",
    "# User nodes\n",
    "nodes_user = nodes.loc[:,['u.id','username','link_karma','comment_karma','total_posts_per_user','Time Interval','total_score_per_user','average_karma_per_post']]\n",
    "nodes_user = nodes_user.rename(columns={'u.id':'Id','username':'Label'})\n",
    "nodes_user['node_type'] = 'user'\n",
    "nodes_user = nodes_user.set_index(keys='Id')\n",
    "# Subreddit nodes\n",
    "nodes_subreddit = nodes.loc[:,['s.id','subreddit','total_posts_per_subreddit','Time Interval','total_scores_per_subreddit']]\n",
    "nodes_subreddit = nodes_subreddit.rename(columns={'s.id':'Id','subreddit':'Label'})\n",
    "nodes_subreddit['node_type'] = 'subreddit'\n",
    "nodes_subreddit = nodes_subreddit.set_index(keys='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30337, 14)\n",
      "(30337, 8)\n",
      "(30337, 5)\n",
      "(30337, 3)\n",
      "(30337, 3)\n"
     ]
    }
   ],
   "source": [
    "# Creating the relationships dataframe\n",
    "relationships = final_graph.loc[:,['u.id','s.id','p.id']]\n",
    "\n",
    "# Processing relationships\n",
    "# User to post relationships\n",
    "rel_user_to_post = nodes.loc[:,['u.id','p.id']]\n",
    "rel_user_to_post['Submitted'] = 'Submitted'\n",
    "rel_user_to_post = rel_user_to_post.rename(columns={'u.id':'Source','p.id':'Target'})\n",
    "\n",
    "# Post to Subreddit relationships\n",
    "rel_post_to_subreddit = nodes.loc[:,['p.id','s.id']]\n",
    "rel_post_to_subreddit['Submitted'] = 'Submitted'\n",
    "rel_post_to_subreddit = rel_post_to_subreddit.rename(columns={'p.id':'Source','s.id':'Target'})\n",
    "\n",
    "\n",
    "print(nodes_post.shape)\n",
    "print(nodes_user.shape)\n",
    "print(nodes_subreddit.shape)\n",
    "print(rel_user_to_post.shape)\n",
    "print(rel_post_to_subreddit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'C:\\\\Users\\\\delah\\\\Documents\\\\Programming\\\\workspace\\\\AAForB_Assignment4\\\\data'\n",
    "ANALYSIS_DIR = '\\\\gephi'\n",
    "os.chdir(DATA_DIR + ANALYSIS_DIR)\n",
    "\n",
    "nodes_post.to_csv('post_nodes.csv')\n",
    "nodes_user.to_csv('user_nodes.csv')\n",
    "nodes_subreddit.to_csv('subreddit_nodes.csv')\n",
    "\n",
    "rel_user_to_post.to_csv('user_to_post_relationships.csv')\n",
    "rel_post_to_subreddit.to_csv('post_to_subreddit_relationships.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subreddits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-54b976d32c47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msubreddits\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'subreddits' is not defined"
     ]
    }
   ],
   "source": [
    "subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEO4J QUERIES\n",
    "## Coronavirus trends\n",
    "\n",
    "**askreddit**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'askreddit'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "\n",
    "**science**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'science'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "\n",
    "**worldnews**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'worldnews'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "\n",
    "**videos**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'videos'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**todayilearned**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'todayilearned'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**news**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'news'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**iama**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'iama'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**askscience**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'askscience'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**explainlikeimfive**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'explainlikeimfive'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**lifeprotips**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'lifeprotips'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**nottheonion**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'nottheonion'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**upliftingnews**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'upliftingnews'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**dataisbeautiful**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'dataisbeautiful'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**technology**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'technology'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**politics**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'politics'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**europe**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'europe'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**coronavirus**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'coronavirus'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**covid19**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'covid19'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n",
    "**china_flu**\n",
    "```\n",
    "MATCH (p:Post)-[e1]-(s:Subreddit{display_name:'china_flu'})\n",
    "WHERE p.title contains 'coronavirus' OR p.title contains 'covid'OR p.title contains 'corona virus'\n",
    "RETURN p.title, s.display_name, p.created_utc_str, p.score\n",
    "LIMIT 100\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
